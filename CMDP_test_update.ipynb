{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xzxstar107/Feature-basedMDP_NV/blob/main/CMDP_test_update.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKUCFyn8Cvf2"
      },
      "outputs": [],
      "source": [
        "!pip install gurobipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O783YYLDC2Ch"
      },
      "outputs": [],
      "source": [
        "# !wget https://packages.gurobi.com/10.0/gurobi10.0.1_linux64.tar.gz\n",
        "# !tar -xvf gurobi10.0.1_linux64.tar.gz\n",
        "# !cd gurobi100/linux64/ && python3 setup.py install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGwb5Ls5DgtP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['GRB_LICENSE_FILE'] = '/content/gurobi.lic'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-UkdDbtDn9d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTsTMedu-_48"
      },
      "outputs": [],
      "source": [
        "def generate_synthetic_data(n, d, n_x, seed=2023):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Create covariance matrix Sigma\n",
        "    Sigma = np.zeros((d, d))\n",
        "    for i in range(d):\n",
        "        for j in range(d):\n",
        "            Sigma[i, j] = (199 / 200) ** abs(i - j)\n",
        "\n",
        "    # Generate X using multivariate normal distribution\n",
        "    X = np.random.multivariate_normal(mean=np.zeros(d), cov=Sigma, size=n)\n",
        "\n",
        "    # Discretize X\n",
        "    X = np.round(X)\n",
        "\n",
        "    # Define beta_0\n",
        "    beta_0 = np.array([((-1) ** i) * (200 - i) for i in range(d)])\n",
        "\n",
        "    # Normalize beta_0 to have L2 norm equal to 25\n",
        "    beta_0 = (beta_0 / np.linalg.norm(beta_0)) * 25\n",
        "\n",
        "    # Generate D values\n",
        "    D = np.zeros((n, n_x))\n",
        "    for i, x in enumerate(X):\n",
        "        for j in range(n_x):\n",
        "            positive_d_count = 0\n",
        "            while positive_d_count < n_x:\n",
        "                epsilon = np.random.normal(0, 1)\n",
        "                d = 1.7*(np.sin(2 * np.dot(x, beta_0)) + 2 * np.exp(-16 * np.dot(x, beta_0) ** 2) + 1) + epsilon\n",
        "\n",
        "                if d > 0:\n",
        "                    D[i, positive_d_count] = d\n",
        "                    positive_d_count += 1\n",
        "\n",
        "    # Discretize D ensuring different values per X\n",
        "    for i in range(n):\n",
        "        unique_d_values = np.unique(np.round(D[i, :]))\n",
        "        while len(unique_d_values) < n_x:\n",
        "            for j in range(n_x):\n",
        "                epsilon = np.random.normal(0, 1)\n",
        "                d = 1.7*(np.sin(2 * np.dot(X[i], beta_0)) + 2 * np.exp(-16 * np.dot(X[i], beta_0) ** 2) + 1) + epsilon\n",
        "\n",
        "                if d > 0:\n",
        "                    D[i, j] = d\n",
        "            unique_d_values = np.unique(np.round(D[i, :]))\n",
        "\n",
        "        D[i, :] = np.round(unique_d_values)\n",
        "\n",
        "    return X, D\n",
        "\n",
        "# an updated version of the generate_synthetic_data function with a more distinct beta_0\n",
        "def generate_synthetic_data(n, d, n_x, seed=2023):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Create covariance matrix Sigma\n",
        "    Sigma = np.zeros((d, d))\n",
        "    for i in range(d):\n",
        "        for j in range(d):\n",
        "            Sigma[i, j] = (199 / 200) ** abs(i - j)\n",
        "\n",
        "    # Generate X using multivariate normal distribution\n",
        "    X = np.random.multivariate_normal(mean=np.zeros(d), cov=Sigma, size=n)\n",
        "\n",
        "    # Discretize X\n",
        "    X = np.round(X)\n",
        "\n",
        "    # Define beta_0\n",
        "    beta_0 = np.array([((-1) ** i) * (400 - i) for i in range(d)])  # Increased the constant\n",
        "\n",
        "    # Normalize beta_0 to have L2 norm equal to 25\n",
        "    beta_0 = (beta_0 / np.linalg.norm(beta_0)) * 25\n",
        "\n",
        "    # Generate D values\n",
        "    D = np.zeros((n, n_x))\n",
        "    for i, x in enumerate(X):\n",
        "        for j in range(n_x):\n",
        "            positive_d_count = 0\n",
        "            while positive_d_count < n_x:\n",
        "                epsilon = np.random.normal(0, 1)\n",
        "                d = 1.7*(np.sin(2 * np.dot(x, beta_0)) + 2 * np.exp(-16 * np.dot(x, beta_0) ** 2) + 1) + epsilon\n",
        "\n",
        "                if d > 0:\n",
        "                    D[i, positive_d_count] = d\n",
        "                    positive_d_count += 1\n",
        "\n",
        "    # Discretize D ensuring different values per X\n",
        "    for i in range(n):\n",
        "        unique_d_values = np.unique(np.round(D[i, :]))\n",
        "        while len(unique_d_values) < n_x:\n",
        "            for j in range(n_x):\n",
        "                epsilon = np.random.normal(0, 1)\n",
        "                d = 1.7*(np.sin(2 * np.dot(X[i], beta_0)) + 2 * np.exp(-16 * np.dot(X[i], beta_0) ** 2) + 1) + epsilon\n",
        "\n",
        "                if d > 0:\n",
        "                    D[i, j] = d\n",
        "            unique_d_values = np.unique(np.round(D[i, :]))\n",
        "\n",
        "        D[i, :] = np.round(unique_d_values)\n",
        "\n",
        "    return X, D\n",
        "\n",
        "\n",
        "# Example usage\n",
        "n = 10\n",
        "d = 5\n",
        "n_x = 3\n",
        "X, D = generate_synthetic_data(n, d, n_x)\n",
        "print(\"X:\\n\", X)\n",
        "print(\"D:\\n\", D, D.shape)\n",
        "print(\"mean of D:\\n\", D.mean(axis=1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bi2eSknjOyxX"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "n = 10\n",
        "d = 5\n",
        "n_x = 5\n",
        "X, D = generate_synthetic_data(n, d, n_x)\n",
        "# D = D**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyYzhMMbO1un"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrfswF4ZO3LQ"
      },
      "outputs": [],
      "source": [
        "D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur1n2C6PHbjk"
      },
      "outputs": [],
      "source": [
        "# Cost function q\n",
        "def q(y, d, h, b):\n",
        "    return h * max(0, -y) + b * max(0, y)\n",
        "\n",
        "def mle(X, D, function_class):\n",
        "    n = len(X)\n",
        "    P_hat = np.zeros((n, len(D[0])))\n",
        "\n",
        "    for i, x in enumerate(X):\n",
        "        log_likelihood = np.zeros(len(function_class))\n",
        "        for j, p in enumerate(function_class):\n",
        "            log_likelihood[j] = np.sum(np.log(p(x, D[i, :])))\n",
        "        best_function_idx = np.argmax(log_likelihood)\n",
        "        P_hat[i, :] = function_class[best_function_idx](x, D[i, :])\n",
        "\n",
        "    return P_hat\n",
        "\n",
        "def value_iteration(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon, P_hat):\n",
        "    n_y = len(Y)\n",
        "    n_x = len(X)\n",
        "    V_yx = np.zeros((n_y, n_x))\n",
        "    V_y = np.zeros(n_y)\n",
        "    V_next_yx = np.zeros_like(V_yx)\n",
        "    V_next_y = np.zeros_like(V_y)\n",
        "    policy = np.zeros_like(V_yx, dtype=int)\n",
        "\n",
        "    # Store the value functions and policies at each iteration\n",
        "    value_history = []\n",
        "    policy_history = []\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        for i, y in enumerate(Y):\n",
        "            for j, x in enumerate(X):\n",
        "                min_val = np.inf\n",
        "                min_a = None\n",
        "\n",
        "                for a in A:\n",
        "                    cost = c * a\n",
        "                    y_plus = y + a - D[j, :]\n",
        "\n",
        "                    # Clip Y_plus values to stay within the Y range\n",
        "                    y_plus = np.clip(y_plus, Y[0], Y[-1])\n",
        "\n",
        "                    # Find the index of Y_plus values in the Y array\n",
        "                    y_plus_idx = np.searchsorted(Y, y_plus)\n",
        "\n",
        "                    # Calculate the expected value of the next state, weighted by P(D|X)\n",
        "                    next_state_value = V_yx[y_plus_idx, j]\n",
        "                    weighted_next_state_value = np.sum(P_hat[j, :] * next_state_value)\n",
        "\n",
        "                    value = cost + q(y, D[j, :].mean(), h, b) + gamma * weighted_next_state_value\n",
        "\n",
        "                    if value < min_val:\n",
        "                        min_val = value\n",
        "                        min_a = a\n",
        "\n",
        "                V_next_yx[i, j] = min_val\n",
        "                policy[i, j] = min_a\n",
        "\n",
        "            V_next_y[i] = np.mean(V_next_yx[i, :])\n",
        "\n",
        "        value_history.append(V_next_y.copy())\n",
        "        policy_history.append(policy.copy())\n",
        "\n",
        "        if np.abs(V_next_y - V_y).max() < epsilon:\n",
        "            break\n",
        "\n",
        "        V_yx = np.copy(V_next_yx)\n",
        "        V_y = np.copy(V_next_y)\n",
        "\n",
        "    return V_y, policy, value_history, policy_history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# include the time stamp\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def value_iteration(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon, P_hat):\n",
        "    n_y = len(Y)\n",
        "    n_x = len(X)\n",
        "    V_yx = np.zeros((n_y, n_x))\n",
        "    V_y = np.zeros(n_y)\n",
        "    V_next_yx = np.zeros_like(V_yx)\n",
        "    V_next_y = np.zeros_like(V_y)\n",
        "    policy = np.zeros_like(V_yx, dtype=int)\n",
        "\n",
        "    # Store the value functions, policies and cumulative time at each iteration\n",
        "    value_history = []\n",
        "    policy_history = []\n",
        "    cumulative_time_history = []\n",
        "\n",
        "    cumulative_time = 0.0\n",
        "    for iteration in range(max_iterations):\n",
        "        start_time = time.time()\n",
        "        for i, y in enumerate(Y):\n",
        "            for j, x in enumerate(X):\n",
        "                min_val = np.inf\n",
        "                min_a = None\n",
        "\n",
        "                for a in A:\n",
        "                    cost = c * a\n",
        "                    y_plus = y + a - D[j, :]\n",
        "\n",
        "                    # Clip Y_plus values to stay within the Y range\n",
        "                    y_plus = np.clip(y_plus, Y[0], Y[-1])\n",
        "\n",
        "                    # Find the index of Y_plus values in the Y array\n",
        "                    y_plus_idx = np.searchsorted(Y, y_plus)\n",
        "\n",
        "                    # Calculate the expected value of the next state, weighted by P(D|X)\n",
        "                    next_state_value = V_yx[y_plus_idx, j]\n",
        "                    weighted_next_state_value = np.sum(P_hat[j, :] * next_state_value)\n",
        "\n",
        "                    value = cost + q(y, D[j, :].mean(), h, b) + gamma * weighted_next_state_value\n",
        "\n",
        "                    if value < min_val:\n",
        "                        min_val = value\n",
        "                        min_a = a\n",
        "\n",
        "                V_next_yx[i, j] = min_val\n",
        "                policy[i, j] = min_a\n",
        "\n",
        "            V_next_y[i] = np.mean(V_next_yx[i, :])\n",
        "\n",
        "        cumulative_time += time.time() - start_time\n",
        "        cumulative_time_history.append(cumulative_time)\n",
        "\n",
        "        value_history.append(V_next_y.copy())\n",
        "        policy_history.append(policy.copy())\n",
        "\n",
        "        if np.abs(V_next_y - V_y).max() < epsilon:\n",
        "            break\n",
        "\n",
        "        V_yx = np.copy(V_next_yx)\n",
        "        V_y = np.copy(V_next_y)\n",
        "\n",
        "    return V_y, policy, value_history, policy_history, cumulative_time_history"
      ],
      "metadata": {
        "id": "J4zvP2MbWySO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhZxJzeZwce_"
      },
      "outputs": [],
      "source": [
        "def generate_synthetic_data(n, d, n_x, seed=2023):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Create covariance matrix Sigma\n",
        "    Sigma = np.zeros((d, d))\n",
        "    for i in range(d):\n",
        "        for j in range(d):\n",
        "            Sigma[i, j] = (199 / 200) ** abs(i - j)\n",
        "\n",
        "    # Generate X using multivariate normal distribution\n",
        "    X = np.random.multivariate_normal(mean=np.zeros(d), cov=Sigma, size=n)\n",
        "\n",
        "    # Discretize X\n",
        "    X = np.round(X)\n",
        "\n",
        "    # Define beta_0\n",
        "    beta_0 = np.array([((-1) ** i) * (200 - i) for i in range(d)])\n",
        "\n",
        "    # Normalize beta_0 to have L2 norm equal to 25\n",
        "    beta_0 = (beta_0 / np.linalg.norm(beta_0)) * 25\n",
        "\n",
        "    # Generate D values\n",
        "    D = np.zeros((n, n_x))\n",
        "    for i, x in enumerate(X):\n",
        "        for j in range(n_x):\n",
        "            positive_d_count = 0\n",
        "            while positive_d_count < n_x:\n",
        "                epsilon = np.random.normal(0, 1)\n",
        "                d = 1.7*(np.sin(2 * np.dot(x, beta_0)) + 2 * np.exp(-16 * np.dot(x, beta_0) ** 2) + 1) + epsilon\n",
        "\n",
        "                if d > 0:\n",
        "                    D[i, positive_d_count] = d\n",
        "                    positive_d_count += 1\n",
        "\n",
        "    # Discretize D ensuring different values per X\n",
        "    for i in range(n):\n",
        "        unique_d_values = np.unique(np.round(D[i, :]))\n",
        "        while len(unique_d_values) < n_x:\n",
        "            for j in range(n_x):\n",
        "                epsilon = np.random.normal(0, 1)\n",
        "                d = 1.7*(np.sin(2 * np.dot(X[i], beta_0)) + 2 * np.exp(-16 * np.dot(X[i], beta_0) ** 2) + 1) + epsilon\n",
        "\n",
        "                if d > 0:\n",
        "                    D[i, j] = d\n",
        "            unique_d_values = np.unique(np.round(D[i, :]))\n",
        "\n",
        "        D[i, :] = np.round(unique_d_values)\n",
        "\n",
        "    return X, D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBm4eDWQtyIV"
      },
      "outputs": [],
      "source": [
        "# Gaussian kernel function\n",
        "def gaussian_kernel(x, D, mu, sigma):\n",
        "    return np.exp(-0.5 * ((D - mu) / sigma) ** 2) / (sigma * np.sqrt(2 * np.pi))\n",
        "\n",
        "# Parametrized function class\n",
        "def parametrized_function(x, D, mu, sigma):\n",
        "    n_d = len(D)\n",
        "    probabilities = np.zeros(n_d)\n",
        "\n",
        "    for i in range(n_d):\n",
        "        probabilities[i] = gaussian_kernel(x, D[i], mu, sigma)\n",
        "\n",
        "    probabilities /= np.sum(probabilities)\n",
        "\n",
        "    return probabilities\n",
        "\n",
        "# Define the parameter grid for the MLE estimation\n",
        "mu_grid = np.linspace(10, 20, 3)\n",
        "sigma_grid = np.linspace(1, 5, 3)\n",
        "\n",
        "# Create a list of possible distribution functions for the MLE estimation\n",
        "function_class = []\n",
        "for mu in mu_grid:\n",
        "    for sigma in sigma_grid:\n",
        "        function_class.append(lambda x, D: parametrized_function(x, D, mu, sigma))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMjeh414vrZ3"
      },
      "outputs": [],
      "source": [
        "# Create synthetic data for X and D\n",
        "n = 20\n",
        "d = 10\n",
        "n_x = 5\n",
        "X, D = generate_synthetic_data(n, d, n_x)\n",
        "\n",
        "# Estimate P(D|X) using the MLE algorithm\n",
        "P_hat = mle(X, D, function_class)\n",
        "print(\"P_hat:\\n\", P_hat)\n",
        "\n",
        "# Value iteration parameters\n",
        "Y = np.arange(-10, 11, 1)\n",
        "A = np.arange(0, 11, 1)\n",
        "gamma = 0.1\n",
        "h = 0.9\n",
        "b = 1\n",
        "c = 0\n",
        "max_iterations = 1000\n",
        "epsilon = 1e-6\n",
        "\n",
        "# # Run the value iteration algorithm\n",
        "# V_y, policy = value_iteration(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon, P_hat)\n",
        "# Run value_iteration function\n",
        "V_y, policy, value_history, policy_history = value_iteration(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon, P_hat)\n",
        "print(\"V_y:\\n\", V_y)\n",
        "print(\"Policy:\\n\", policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlApr5UCHtsd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, x in enumerate(X):\n",
        "    for j, y in enumerate(Y):\n",
        "        results.append([x, y, V_y[j], D.mean(axis=1)[i], policy[j, i]])\n",
        "\n",
        "results_df = pd.DataFrame(results, columns=[\"X\", \"Y\", \"V\", \"meanD\", \"policy\"])\n",
        "results_df.to_csv(\"value_iteration_results\"+str(h)+str(b)+str(c)+\"_nx\"+str(n)+str(gamma)+\".csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWZA5BZDr-zP"
      },
      "outputs": [],
      "source": [
        "\"value_iteration_results\"+str(h)+str(b)+str(c)+\".csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uqn8HSEa6EY"
      },
      "source": [
        "## Plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ego9ETC7bgQ3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j89Bw88bfY2d"
      },
      "outputs": [],
      "source": [
        "# Define your parameter ranges\n",
        "h_values = [1, 2]\n",
        "b_values = [1, 2]\n",
        "c_values = [0, 1]\n",
        "gamma_values = [0.1, 0.9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_HVdo5GZkPt"
      },
      "outputs": [],
      "source": [
        "def plot_policy(x, i, D, Y, policy, h, b, c, gamma, file_name):\n",
        "    fig, ax = plt.subplots(figsize=(10,6))\n",
        "    ax.plot(Y, policy[:, i])\n",
        "    ax.set_xlabel('Inventory Level')\n",
        "    ax.set_ylabel('Order Quantity')\n",
        "    mean_D = D.mean(axis=1)[i]\n",
        "    plot_title = f\"Policy for X{i} = {x}, Mean D(X) = {mean_D:.2f}\"\n",
        "    ax.set_title(plot_title)\n",
        "    ax.grid()\n",
        "\n",
        "    # Add parameter values as footnote\n",
        "    footnote = f'h={h}, b={b}, c={c}, gamma={gamma}'\n",
        "    ax.annotate(footnote, xy=(0.2, -0.1), xycoords='axes fraction', fontsize=10, ha='center', va='bottom')\n",
        "\n",
        "    # Adjust subplot layout to accommodate the footnote\n",
        "    plt.subplots_adjust(bottom=0.1)\n",
        "\n",
        "    plt.savefig(file_name)\n",
        "    plt.close(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eB0-xyxsZ1qU"
      },
      "outputs": [],
      "source": [
        "# Define your parameter ranges\n",
        "h_values = [1, 2]\n",
        "b_values = [1, 2]\n",
        "c_values = [0, 1]\n",
        "gamma_values = [0.1, 0.9]\n",
        "\n",
        "for h in h_values:\n",
        "    for b in b_values:\n",
        "        for c in c_values:\n",
        "            for gamma in gamma_values:\n",
        "                subdir = f\"h={h}_b={b}_c={c}_gamma={gamma}\"\n",
        "                os.makedirs(subdir, exist_ok=True)\n",
        "                for i, x in enumerate(X):\n",
        "                    # V_y, policy = value_iteration(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon, P_hat)\n",
        "                    V_y, policy, value_history, policy_history = value_iteration(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon, P_hat)\n",
        "                    file_name = f\"{subdir}/policy_X{i}={x}_h={h}_b={b}_c={c}_gamma={gamma}.png\"\n",
        "                    plot_policy(x, i, D[:, i:i+1], Y, policy, h, b, c, gamma, file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Aoi7Ddd2GY0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compare_policies(X, D, Y, A, policy, h, b, c, gamma):\n",
        "    fig = plt.figure(figsize=(10, 6))  # Set figure size\n",
        "    ax = fig.add_axes([0.1, 0.1, 0.6, 0.8])  # Adjust main plot position and size\n",
        "\n",
        "    for i, x in enumerate(X):\n",
        "        ax.plot(Y, policy[:, i], label=f\"X{i}, Mean D(X) = {D.mean(axis=1)[i]:.2f}\")\n",
        "\n",
        "    ax.set_xlabel('Inventory Level')\n",
        "    ax.set_ylabel('Order Quantity')\n",
        "    ax.set_title('Optimal Policy under Different Context')\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)  # Adjust legend position\n",
        "\n",
        "    # Add parameter values as footnote\n",
        "    footnote = f'h={h}, b={b}, c={c}, gamma={gamma}'\n",
        "    ax.annotate(footnote, xy=(0.2, -0.1), xycoords='axes fraction', fontsize=10, ha='center', va='bottom')\n",
        "\n",
        "    # Adjust subplot layout to accommodate the footnote\n",
        "    plt.subplots_adjust(bottom=0.5)\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBAjCRXe0hPh"
      },
      "outputs": [],
      "source": [
        "# Unique policies\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compare_policies(X, D, Y, A, policy, h, b, c, gamma):\n",
        "    fig = plt.figure(figsize=(10, 6))  # Set figure size\n",
        "    ax = fig.add_axes([0.1, 0.1, 0.6, 0.8])  # Adjust main plot position and size\n",
        "\n",
        "    unique_policies = {}\n",
        "    for i, x in enumerate(X):\n",
        "        # Check if the policy is unique (not already plotted)\n",
        "        policy_str = np.array2string(policy[:, i], separator=',')\n",
        "        if policy_str not in unique_policies:\n",
        "            unique_policies[policy_str] = True\n",
        "            ax.plot(Y, policy[:, i], label=f\"X{i}, Mean D(X) = {D.mean(axis=1)[i]:.2f}\")\n",
        "\n",
        "    ax.set_xlabel('Inventory Level')\n",
        "    ax.set_ylabel('Order Quantity')\n",
        "    ax.set_title('Optimal Policy under Different Context')\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)  # Adjust legend position\n",
        "\n",
        "    # Add parameter values as footnote\n",
        "    footnote = f'h={h}, b={b}, c={c}, gamma={gamma}'\n",
        "    ax.annotate(footnote, xy=(0.2, -0.1), xycoords='axes fraction', fontsize=10, ha='center', va='bottom')\n",
        "\n",
        "    # Adjust subplot layout to accommodate the footnote\n",
        "    plt.subplots_adjust(bottom=0.5)\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kGKYgl10myD"
      },
      "outputs": [],
      "source": [
        "# Define your parameter ranges\n",
        "h_values = [1, 2]\n",
        "b_values = [1, 2]\n",
        "c_values = [0, 1]\n",
        "gamma_values = [0.1, 0.9]\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "output_dir = \"policy_comparison_plots2\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for h in h_values:\n",
        "    for b in b_values:\n",
        "        for c in c_values:\n",
        "            for gamma in gamma_values:\n",
        "                # Call your value_iteration function to get the policy for this combination of parameters\n",
        "                # V_y, policy = value_iteration(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon, P_hat)\n",
        "\n",
        "                # Create and save the plot\n",
        "                fig = compare_policies(X, D, Y, A, policy, h, b, c, gamma)\n",
        "                fig.savefig(os.path.join(output_dir, f'policy_comparison_h_{h}_b_{b}_c_{c}_gamma_{gamma}.png'))\n",
        "                plt.close(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBwEha38nXnv"
      },
      "outputs": [],
      "source": [
        "def plot_value_function_convergence(value_history):\n",
        "    num_iterations = len(value_history)\n",
        "\n",
        "    plt.figure()\n",
        "    value_differences = [np.abs(value_history[i+1] - value_history[i]).max() for i in range(num_iterations - 1)]\n",
        "    plt.plot(range(1, num_iterations), value_differences)\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Max Absolute Change in Value Function')\n",
        "    plt.title('Value Function Convergence')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "def plot_policy_convergence(policy_history):\n",
        "    num_iterations = len(policy_history)\n",
        "\n",
        "    plt.figure()\n",
        "    policy_differences = [np.sum(policy_history[i+1] != policy_history[i]) for i in range(num_iterations - 1)]\n",
        "    plt.plot(range(1, num_iterations), policy_differences)\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Number of Policy Changes')\n",
        "    plt.title('Policy Convergence')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "def plot_combined_convergence(value_history, policy_history):\n",
        "    num_iterations = len(value_history)\n",
        "\n",
        "    fig, ax1 = plt.subplots()\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    value_differences = [np.abs(value_history[i+1] - value_history[i]).max() for i in range(num_iterations - 1)]\n",
        "    ax1.plot(range(1, num_iterations), value_differences, 'b-')\n",
        "    ax1.set_xlabel('Iteration')\n",
        "    ax1.set_ylabel('Max Absolute Change in Value Function', color='b')\n",
        "    ax1.tick_params(axis='y', labelcolor='b')\n",
        "    ax1.grid()\n",
        "\n",
        "    policy_differences = [np.sum(policy_history[i+1] != policy_history[i]) for i in range(num_iterations - 1)]\n",
        "    ax2.plot(range(1, num_iterations), policy_differences, 'r-')\n",
        "    ax2.set_ylabel('Number of Policy Changes', color='r')\n",
        "    ax2.tick_params(axis='y', labelcolor='r')\n",
        "\n",
        "    plt.title('Value Function and Policy Convergence')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jzQPMlaJV7fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWDd0KHOoYxO"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Define your parameter ranges\n",
        "h_values = [1, 2]\n",
        "b_values = [1, 2]\n",
        "c_values = [0, 1]\n",
        "gamma_values = [0.1, 0.9]\n",
        "\n",
        "# Initialize dataframe to store results\n",
        "df = pd.DataFrame(columns=['Algorithm', 'h', 'b', 'c', 'gamma', 'Time', 'Converged_Iteration'])\n",
        "\n",
        "# Iterate over each value iteration algorithm\n",
        "for value_iteration in [value_iteration, value_iteration0, value_iteration2]:\n",
        "\n",
        "    # Iterate over each hyperparameter combination\n",
        "    for h in h_values:\n",
        "        for b in b_values:\n",
        "            for c in c_values:\n",
        "                for gamma in gamma_values:\n",
        "\n",
        "                    start_time = time.time()\n",
        "\n",
        "                    # Run value iteration algorithm and get results\n",
        "                    V_y, policy, value_history, policy_history = value_iteration(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon, P_hat)\n",
        "\n",
        "                    end_time = time.time()\n",
        "\n",
        "                    computation_time = end_time - start_time\n",
        "\n",
        "                    converged_iteration = len(value_history)\n",
        "\n",
        "                    # Save results to dataframe\n",
        "                    df = df.append({\n",
        "                        'Algorithm': value_iteration.__name__,\n",
        "                        'h': h,\n",
        "                        'b': b,\n",
        "                        'c': c,\n",
        "                        'gamma': gamma,\n",
        "                        'Time': computation_time,\n",
        "                        'Converged_Iteration': converged_iteration\n",
        "                    }, ignore_index=True)\n",
        "\n",
        "                    # Plot results\n",
        "                    plot_value_function_convergence(value_history)\n",
        "                    plt.savefig(f\"{value_iteration.__name__}_h{h}_b{b}_c{c}_gamma{gamma}_value_convergence.png\")\n",
        "\n",
        "                    plot_policy_convergence(policy_history)\n",
        "                    plt.savefig(f\"{value_iteration.__name__}_h{h}_b{b}_c{c}_gamma{gamma}_policy_convergence.png\")\n",
        "\n",
        "                    plot_combined_convergence(value_history, policy_history)\n",
        "                    plt.savefig(f\"{value_iteration.__name__}_h{h}_b{b}_c{c}_gamma{gamma}_combined_convergence.png\")\n",
        "\n",
        "# Save dataframe to csv\n",
        "df.to_csv(\"value_iteration_results.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over all parameter combinations of h, b, c, gamma, and X\n",
        "for h in h_values:\n",
        "    for b in b_values:\n",
        "        for c in c_values:\n",
        "            for gamma in gamma_values:\n",
        "                subdir = f\"h={h}_b={b}_c={c}_gamma={gamma}\"\n",
        "                os.makedirs(subdir, exist_ok=True)\n",
        "\n",
        "                results_dir = f\"{subdir}/results\"\n",
        "                os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "                fig_dir = f\"{subdir}/figures\"\n",
        "                os.makedirs(fig_dir, exist_ok=True)\n",
        "\n",
        "                for i, x in enumerate(X):\n",
        "                    # Run the first value iteration function\n",
        "                    V_y1, policy1, value_history1, policy_history1 = value_iteration(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon, P_hat)\n",
        "                    # Run the second value iteration function\n",
        "                    V_y2, policy2, value_history2, policy_history2 = value_iteration2(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon)\n",
        "                    # Run the third value iteration function\n",
        "                    V_y0, policy0, value_history0, policy_history0 = value_iteration0(D, Y, A, gamma, h, b, c, max_iterations, epsilon)\n",
        "\n",
        "                    np.save(f\"{results_dir}/V_y1_X{i}_h={h}_b={b}_c={c}_gamma={gamma}.npy\", V_y1)\n",
        "                    np.save(f\"{results_dir}/policy1_X{i}_h={h}_b={b}_c={c}_gamma={gamma}.npy\", policy1)\n",
        "                    np.save(f\"{results_dir}/V_y2_X{i}_h={h}_b={b}_c={c}_gamma={gamma}.npy\", V_y2)\n",
        "                    np.save(f\"{results_dir}/policy2_X{i}_h={h}_b={b}_c={c}_gamma={gamma}.npy\", policy2)\n",
        "                    np.save(f\"{results_dir}/V_y0_X{i}_h={h}_b={b}_c={c}_gamma={gamma}.npy\", V_y0)\n",
        "                    np.save(f\"{results_dir}/policy0_X{i}_h={h}_b={b}_c={c}_gamma={gamma}.npy\", policy0)\n",
        "\n",
        "                    plot_convergence(i, value_history1, value_history2, value_history0, h, b, c, gamma, fig_dir)"
      ],
      "metadata": {
        "id": "sLuv6u4uWMe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function for plotting value function convergence for each algorithm\n",
        "def plot_value_function_convergence(value_history, function_name):\n",
        "    num_iterations = len(value_history)\n",
        "\n",
        "    plt.figure()\n",
        "    value_differences = [np.abs(value_history[i+1] - value_history[i]).max() for i in range(num_iterations - 1)]\n",
        "    plt.plot(range(1, num_iterations), value_differences, label=function_name)\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Max Absolute Change in Value Function')\n",
        "    plt.title('Value Function Convergence')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Initialize dataframe to store results\n",
        "df = pd.DataFrame(columns=['Algorithm', 'h', 'b', 'c', 'gamma', 'Time', 'Converged_Iteration'])\n",
        "\n",
        "# Initialize a list to store the value histories\n",
        "value_histories = []\n",
        "function_names = []\n",
        "\n",
        "# Iterate over each value iteration algorithm\n",
        "for value_iteration in [value_iteration, value_iteration0, value_iteration2]:\n",
        "\n",
        "    # Iterate over each hyperparameter combination\n",
        "    for h in h_values:\n",
        "        for b in b_values:\n",
        "            for c in c_values:\n",
        "                for gamma in gamma_values:\n",
        "\n",
        "                    # Run value iteration algorithm and get results\n",
        "                    V_y, policy, value_history, policy_history = value_iteration(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon, P_hat)\n",
        "\n",
        "                    computation_time = time_history[-1]\n",
        "\n",
        "                    converged_iteration = len(value_history)\n",
        "\n",
        "                    # Save results to dataframe\n",
        "                    df = df.append({\n",
        "                        'Algorithm': value_iteration.__name__,\n",
        "                        'h': h,\n",
        "                        'b': b,\n",
        "                        'c': c,\n",
        "                        'gamma': gamma,\n",
        "                        'Time': computation_time,\n",
        "                        'Converged_Iteration': converged_iteration\n",
        "                    }, ignore_index=True)\n",
        "\n",
        "                    # Save value history and function name for later plotting\n",
        "                    value_histories.append(value_history)\n",
        "                    function_names.append(value_iteration.__name__)\n",
        "\n",
        "# Now plot the convergence of each function\n",
        "for i in range(len(value_histories)):\n",
        "    plot_value_function_convergence(value_histories[i], function_names[i])\n",
        "\n",
        "# Save dataframe to csv\n",
        "df.to_csv(\"value_iteration_results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "Qm5qpLqqW-9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5VSWzKFpLoK"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a list of the functions for easy iteration\n",
        "value_iterations = [value_iteration, value_iteration0, value_iteration2]\n",
        "\n",
        "# Initialize a dictionary to store the value histories and time histories for each algorithm\n",
        "results = {}\n",
        "\n",
        "for value_iteration in value_iterations:\n",
        "    # Initialize the value history and time history for this algorithm\n",
        "    value_history = []\n",
        "    time_history = []\n",
        "\n",
        "    # Assume all parameters are predefined and same for all algorithms\n",
        "    V_y, policy, vh, ph = value_iteration(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon, P_hat)\n",
        "\n",
        "    # Run the value iteration algorithm\n",
        "    for iteration in range(max_iterations):\n",
        "        # Record the start time of the iteration\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Run the value iteration\n",
        "        V_y, policy, vh_i, ph_i = value_iteration(X, D, Y, A, gamma, h, b, c, iteration, epsilon, P_hat)\n",
        "\n",
        "        # Record the end time of the iteration\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Store the value history and the computation time of this iteration\n",
        "        value_history.append(vh_i)\n",
        "        time_history.append(end_time - start_time)\n",
        "\n",
        "    # Store the value history and time history for this algorithm\n",
        "    results[value_iteration.__name__] = (value_history, time_history)\n",
        "\n",
        "# Plot the value function convergence versus computation time for each algorithm\n",
        "for algorithm, (value_history, time_history) in results.items():\n",
        "    plt.figure()\n",
        "\n",
        "    # Plot value function convergence versus computation time\n",
        "    for i in range(1, len(value_history)):\n",
        "        value_difference = np.abs(value_history[i] - value_history[i-1]).max()\n",
        "        plt.plot(time_history[i], value_difference, label=f'Iteration {i}')\n",
        "\n",
        "    plt.xlabel('Computation Time (s)')\n",
        "    plt.ylabel('Max Absolute Change in Value Function')\n",
        "    plt.title(f'Value Function Convergence for {algorithm}')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define your parameter ranges\n",
        "h_values = [1, 2]\n",
        "b_values = [1, 2]\n",
        "c_values = [0, 1]\n",
        "gamma_values = [0.1, 0.9]"
      ],
      "metadata": {
        "id": "1HIxFBfrA46J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_convergence(i, time_history0, time_history1, time_history2, value_history0, value_history1, value_history2, h, b, c, gamma, fig_dir):\n",
        "    plt.figure(figsize=(10, 7))\n",
        "\n",
        "    if len(time_history0) == len(value_history0):\n",
        "        plt.plot(time_history0, [V[-1] for V in value_history0], label=\"VI 0\")\n",
        "\n",
        "    if len(time_history1) == len(value_history1):\n",
        "        plt.plot(time_history1, [V[-1] for V in value_history1], label=\"VI 1\")\n",
        "\n",
        "    if len(time_history2) == len(value_history2):\n",
        "        plt.plot(time_history2, [V[-1] for V in value_history2], label=\"VI 2\")\n",
        "\n",
        "    plt.xlabel(\"Elapsed Time (s)\")\n",
        "    plt.ylabel(\"Last Value Function\")\n",
        "    plt.title(f\"Value Function Convergence over Time for X{i}, h={h}, b={b}, c={c}, gamma={gamma}\")\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"{fig_dir}/value_convergence_over_time_X{i}_h={h}_b={b}_c={c}_gamma={gamma}.png\")\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "EXtjy3hmDgW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZYRL4rxqC6s"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Initialize a dictionary to store the results for each algorithm and hyperparameters combination\n",
        "results = {}\n",
        "\n",
        "# Run each value iteration function separately\n",
        "for h in h_values:\n",
        "    for b in b_values:\n",
        "        for c in c_values:\n",
        "            for gamma in gamma_values:\n",
        "                for func in ['value_iteration', 'value_iteration0', 'value_iteration2']:\n",
        "                    # Initialize the value history and time history for this algorithm\n",
        "                    value_history = []\n",
        "                    time_history = []\n",
        "\n",
        "                    # Run the value iteration algorithm\n",
        "                    for iteration in range(max_iterations):\n",
        "                        # Record the start time of the iteration\n",
        "                        start_time = time.time()\n",
        "\n",
        "                        # Run the value iteration\n",
        "                        if func == 'value_iteration':\n",
        "                            V_y, policy, vh_i, ph_i = value_iteration(X, D, Y, A, gamma, h, b, c, iteration, epsilon, P_hat)\n",
        "                        elif func == 'value_iteration0':\n",
        "                            V_y, policy, vh_i, ph_i = value_iteration0(D, Y, A, gamma, h, b, c, iteration, epsilon)\n",
        "                        elif func == 'value_iteration2':\n",
        "                            V_y, policy, vh_i, ph_i = value_iteration2(X, D, Y, A, gamma, h, b, c, iteration, epsilon)\n",
        "\n",
        "                        # Record the end time of the iteration\n",
        "                        end_time = time.time()\n",
        "\n",
        "                        # Store the value history and the computation time of this iteration\n",
        "                        value_history.append(vh_i)\n",
        "                        time_history.append(end_time - start_time)\n",
        "\n",
        "                    # Store the value history and time history for this algorithm\n",
        "                    results[(func, h, b, c, gamma)] = (value_history, time_history)\n",
        "\n",
        "# Convert results to a pandas DataFrame and save as csv\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('value_iteration_results.csv')\n",
        "\n",
        "# Create a figure for the plots\n",
        "plt.figure(figsize=(10,8))\n",
        "\n",
        "# Plot the value function convergence versus computation time for each algorithm and hyperparameters combination\n",
        "for (algorithm, h, b, c, gamma), (value_history, time_history) in results.items():\n",
        "\n",
        "    # Plot value function convergence versus computation time\n",
        "    value_differences = [np.abs(value_history[i] - value_history[i-1]).max() for i in range(1, len(value_history))]\n",
        "    plt.plot(time_history[1:], value_differences, label=f'{algorithm}, h={h}, b={b}, c={c}, gamma={gamma}')\n",
        "\n",
        "plt.xlabel('Computation Time (s)')\n",
        "plt.ylabel('Max Absolute Change in Value Function')\n",
        "plt.title('Value Function Convergence Comparison')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.savefig('value_function_convergence_comparison.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_convergence(i, cumulative_time_history0, cumulative_time_history1, cumulative_time_history2, value_history0, value_history1, value_history2, h, b, c, gamma, fig_dir):\n",
        "    plt.figure(figsize=(10, 7))\n",
        "\n",
        "    if len(cumulative_time_history0) == len(value_history0):\n",
        "        plt.plot(cumulative_time_history0, [22-V[-1] for V in value_history0], label=\"Our\")\n",
        "\n",
        "    if len(cumulative_time_history1) == len(value_history1):\n",
        "        plt.plot(cumulative_time_history1, [22-V[-1] for V in value_history1], label=\"gMDP\")\n",
        "\n",
        "    if len(cumulative_time_history2) == len(value_history2):\n",
        "        plt.plot(cumulative_time_history2, [22-V[-1] for V in value_history2], label=\"cMDP\")\n",
        "\n",
        "    plt.xlabel(\"Cumulative Elapsed Time (s)\")\n",
        "    plt.ylabel(\"Value Function\")\n",
        "    plt.title(f\"Value Function Convergence over Time for X{i}, h={h}, b={b}, c={c}, gamma={gamma}\")\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"{fig_dir}/value_convergence_over_time_X{i}_h={h}_b={b}_c={c}_gamma={gamma}.png\")\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "G_9w0o1QgcGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over all parameter combinations of h, b, c, gamma, and X\n",
        "for h in h_values:\n",
        "    for b in b_values:\n",
        "        for c in c_values:\n",
        "            for gamma in gamma_values:\n",
        "                subdir = f\"h={h}_b={b}_c={c}_gamma={gamma}\"\n",
        "                os.makedirs(subdir, exist_ok=True)\n",
        "\n",
        "                results_dir = f\"{subdir}/results\"\n",
        "                os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "                fig_dir = f\"{subdir}/figures\"\n",
        "                os.makedirs(fig_dir, exist_ok=True)\n",
        "\n",
        "                for i, x in enumerate(X):\n",
        "                    # Run the three value iteration functions\n",
        "                    V_y, policy, value_history, policy_history, cumulative_time_history = value_iteration(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon, P_hat)\n",
        "                    V_y0, policy0, value_history0, policy_history0, cumulative_time_history0 = value_iteration0(D, Y, A, gamma, h, b, c, max_iterations, epsilon)\n",
        "                    V_y2, policy2, value_history2, policy_history2, cumulative_time_history2 = value_iteration2(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon)\n",
        "\n",
        "                    plot_convergence(i, cumulative_time_history0, cumulative_time_history, cumulative_time_history2, value_history0, value_history, value_history2, h, b, c, gamma, fig_dir)\n"
      ],
      "metadata": {
        "id": "Nf96CKaiDs0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_value_function_convergence(value_histories, labels):\n",
        "    plt.figure()\n",
        "\n",
        "    for value_history, label in zip(value_histories, labels):\n",
        "        num_iterations = len(value_history)\n",
        "        value_differences = [np.abs(value_history[i+1] - value_history[i]).max() for i in range(num_iterations - 1)]\n",
        "        plt.plot(range(1, num_iterations), value_differences, label=label)\n",
        "\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Max Absolute Change in Value Function')\n",
        "    plt.title('Value Function Convergence')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "RmghokBgo6-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dataframe to store results and lists to hold value histories\n",
        "df = pd.DataFrame(columns=['Algorithm', 'h', 'b', 'c', 'gamma', 'Time', 'Converged_Iteration'])\n",
        "\n",
        "# Let's say that you have three value iteration functions: value_iteration1, value_iteration2, value_iteration3\n",
        "value_histories_1 = []\n",
        "value_histories_2 = []\n",
        "value_histories_3 = []\n",
        "\n",
        "# Iterate over each value iteration algorithm\n",
        "value_iterations = [value_iteration, value_iteration0, value_iteration2]\n",
        "for idx, value_iteration in enumerate(value_iterations):\n",
        "\n",
        "    # Iterate over each hyperparameter combination\n",
        "    for h in h_values:\n",
        "        for b in b_values:\n",
        "            for c in c_values:\n",
        "                for gamma in gamma_values:\n",
        "\n",
        "                    # Run value iteration algorithm and get results\n",
        "                    V_y, policy, value_history, policy_history = value_iteration(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon, P_hat)\n",
        "\n",
        "                    computation_time = time_history[-1]\n",
        "\n",
        "                    converged_iteration = len(value_history)\n",
        "\n",
        "                    # Save results to dataframe\n",
        "                    df = df.append({\n",
        "                        'Algorithm': value_iteration.__name__,\n",
        "                        'h': h,\n",
        "                        'b': b,\n",
        "                        'c': c,\n",
        "                        'gamma': gamma,\n",
        "                        'Time': computation_time,\n",
        "                        'Converged_Iteration': converged_iteration\n",
        "                    }, ignore_index=True)\n",
        "\n",
        "                    # Save value history depending on the value iteration function\n",
        "                    if idx == 0:\n",
        "                        value_histories_1.append(value_history)\n",
        "                    elif idx == 1:\n",
        "                        value_histories_2.append(value_history)\n",
        "                    elif idx == 2:\n",
        "                        value_histories_3.append(value_history)\n",
        "\n",
        "# Call the plotting function after all computations are complete\n",
        "plot_value_function_convergence([value_histories_1, value_histories_2, value_histories_3], ['value_iteration1', 'value_iteration2', 'value_iteration3'])\n",
        "\n",
        "# Save dataframe to csv\n",
        "df.to_csv(\"value_iteration_results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "IK65DS22o568"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Define your parameter ranges\n",
        "h_values = [1, 2]\n",
        "b_values = [1, 2]\n",
        "c_values = [0, 1]\n",
        "gamma_values = [0.1, 0.9]\n",
        "\n",
        "# Initialize dataframe to store results\n",
        "df = pd.DataFrame(columns=['Algorithm', 'h', 'b', 'c', 'gamma', 'Time', 'Converged_Iteration'])\n",
        "\n",
        "# Store value function differences for all algorithms\n",
        "value_function_differences = []\n",
        "\n",
        "# Iterate over each hyperparameter combination\n",
        "for h in h_values:\n",
        "    for b in b_values:\n",
        "        for c in c_values:\n",
        "            for gamma in gamma_values:\n",
        "\n",
        "                # Handle value_iteration\n",
        "                start_time = time.time()\n",
        "                V_y, policy, value_history, policy_history,_ = value_iteration(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon, P_hat)\n",
        "                end_time = time.time()\n",
        "                computation_time = end_time - start_time\n",
        "                converged_iteration = len(value_history)\n",
        "                df = df.append({\n",
        "                    'Algorithm': value_iteration.__name__,\n",
        "                    'h': h,\n",
        "                    'b': b,\n",
        "                    'c': c,\n",
        "                    'gamma': gamma,\n",
        "                    'Time': computation_time,\n",
        "                    'Converged_Iteration': converged_iteration\n",
        "                }, ignore_index=True)\n",
        "                value_function_differences.append({'name': 'value_iteration', 'values': [np.abs(value_history[i+1] - value_history[i]).max() for i in range(len(value_history) - 1)]})\n",
        "\n",
        "# Save dataframe to csv\n",
        "df.to_csv(\"value_iteration_results.csv\", index=False)\n",
        "\n",
        "# Plot value function convergence for all functions\n",
        "plt.figure()\n",
        "for diff in value_function_differences:\n",
        "    plt.plot(range(1, len(diff['values'])+1), diff['values'], label=diff['name'])\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Max Absolute Change in Value Function')\n",
        "plt.title('Value Function Convergence')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vmlXDtVJqqAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_value_function_convergence(value_histories, labels):\n",
        "    plt.figure()\n",
        "\n",
        "    for value_history, label in zip(value_histories, labels):\n",
        "        num_iterations = len(value_history)\n",
        "        value_differences = [np.abs(value_history[i+1] - value_history[i]).max() for i in range(num_iterations - 1)]\n",
        "        plt.plot(range(1, num_iterations), value_differences, label=label)\n",
        "\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Max Absolute Change in Value Function')\n",
        "    plt.title('Value Function Convergence')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "AO8-R8cvudqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dataframe to store results and lists to hold value histories\n",
        "df = pd.DataFrame(columns=['Algorithm', 'h', 'b', 'c', 'gamma', 'Time', 'Converged_Iteration'])\n",
        "\n",
        "# Let's say that you have three value iteration functions: value_iteration1, value_iteration2, value_iteration3\n",
        "value_histories_1 = []\n",
        "value_histories_2 = []\n",
        "value_histories_3 = []\n",
        "\n",
        "# Iterate over each value iteration algorithm\n",
        "value_iterations = [value_iteration, value_iteration0, value_iteration2]\n",
        "for idx, value_iteration in enumerate(value_iterations):\n",
        "\n",
        "    # Iterate over each hyperparameter combination\n",
        "    for h in h_values:\n",
        "        for b in b_values:\n",
        "            for c in c_values:\n",
        "                for gamma in gamma_values:\n",
        "\n",
        "                    # Run value iteration algorithm and get results\n",
        "                    V_y, policy, value_history, policy_history,_ = value_iteration(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon, P_hat)\n",
        "\n",
        "                    computation_time = time_history[-1]\n",
        "\n",
        "                    converged_iteration = len(value_history)\n",
        "\n",
        "                    # Save results to dataframe\n",
        "                    df = df.append({\n",
        "                        'Algorithm': value_iteration.__name__,\n",
        "                        'h': h,\n",
        "                        'b': b,\n",
        "                        'c': c,\n",
        "                        'gamma': gamma,\n",
        "                        'Time': computation_time,\n",
        "                        'Converged_Iteration': converged_iteration\n",
        "                    }, ignore_index=True)\n",
        "\n",
        "                    # Save value history depending on the value iteration function\n",
        "                    if idx == 0:\n",
        "                        value_histories_1.append(value_history)\n",
        "                    elif idx == 1:\n",
        "                        value_histories_2.append(value_history)\n",
        "                    elif idx == 2:\n",
        "                        value_histories_3.append(value_history)\n",
        "\n",
        "# Call the plotting function after all computations are complete\n",
        "plot_value_function_convergence([value_histories_1, value_histories_2, value_histories_3], ['value_iteration1', 'value_iteration2', 'value_iteration3'])\n",
        "\n",
        "# Save dataframe to csv\n",
        "df.to_csv(\"value_iteration_results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "jzXzXfMauel0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over all parameter combinations of h, b, c, gamma, and X\n",
        "for h in h_values:\n",
        "    for b in b_values:\n",
        "        for c in c_values:\n",
        "            for gamma in gamma_values:\n",
        "                subdir = f\"h={h}_b={b}_c={c}_gamma={gamma}\"\n",
        "                os.makedirs(subdir, exist_ok=True)\n",
        "\n",
        "                results_dir = f\"{subdir}/results\"\n",
        "                os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "                fig_dir = f\"{subdir}/figures\"\n",
        "                os.makedirs(fig_dir, exist_ok=True)\n",
        "\n",
        "                for i, x in enumerate(X):\n",
        "                    # Run the three value iteration functions\n",
        "                    V_y, policy, value_history, policy_history, cumulative_time_history = value_iteration(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon, P_hat)\n",
        "                    V_y0, policy0, value_history0, policy_history0, cumulative_time_history0 = value_iteration0(D, Y, A, gamma, h, b, c, max_iterations, epsilon)\n",
        "                    V_y2, policy2, value_history2, policy_history2, cumulative_time_history2 = value_iteration2(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon)\n",
        "\n",
        "                    # # Save the results\n",
        "                    # for vi, time_history, value_history in zip(range(3), [time_history0, time_history1, time_history2], [value_history0, value_history1, value_history2]):\n",
        "                    #     print(len(time_history), len(value_history))  # Add this line\n",
        "                        # df = pd.DataFrame({'time': time_history, 'value': [np.max(V) for V in value_history]})\n",
        "                        # df.to_csv(f\"{results_dir}/VI{vi}_X{i}_h={h}_b={b}_c={c}_gamma={gamma}.csv\", index=False)\n",
        "\n",
        "                    # for vi, time_history, value_history in zip(range(3), [time_history0, time_history1, time_history2], [value_history0, value_history1, value_history2]):\n",
        "                    #     df = pd.DataFrame({'time': time_history, 'value': [np.max(V) for V in value_history]})\n",
        "                    #     df.to_csv(f\"{results_dir}/VI{vi}_X{i}_h={h}_b={b}_c={c}_gamma={gamma}.csv\", index=False)\n",
        "                    # value_history0, value_history, value_history2 = 22-value_history0, 22-value_history, 22-value_history2\n",
        "\n",
        "                    value_histories = [value_history, value_history0, value_history2]\n",
        "                    labels = [\"Our\", \"gMDP\", \"cMDP\"]\n",
        "                    plot_value_function_convergence(value_histories, labels)\n"
      ],
      "metadata": {
        "id": "sUHA0NNsu6cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSWWsExpbWpK"
      },
      "source": [
        "# Save the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__El7WLT2ciA"
      },
      "outputs": [],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIW0AfRV2Y7x"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Set the path of the folder containing the files to download\n",
        "folder_path = '/content/policy_comparison_plots'\n",
        "\n",
        "# Get a list of all files in the folder\n",
        "file_list = os.listdir(folder_path)\n",
        "\n",
        "# Download each file in the folder\n",
        "for file_name in file_list:\n",
        "    files.download(os.path.join(folder_path, file_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcOADHaoJp85"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Set the path of the folder to download\n",
        "folder_path = '/content'\n",
        "\n",
        "# Zip the folder and save as a file\n",
        "shutil.make_archive('policy_plots', 'zip', folder_path)\n",
        "\n",
        "# Download the zipped folder\n",
        "files.download('policy_plots.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVq2LkTweyzi"
      },
      "source": [
        "## vanilla CMDP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5f-xuVzeyPb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Cost function q\n",
        "def q(y, d, h, b):\n",
        "    return h * max(0, -y) + b * max(0, y)\n",
        "\n",
        "# Define the transition probabilities functions\n",
        "def transition_probabilities(X, D):\n",
        "    function_class = []\n",
        "    # TODO: Define the function_class based on your specific transition probabilities\n",
        "    for mu in mu_grid:\n",
        "        for sigma in sigma_grid:\n",
        "            function_class.append(lambda x, D: parametrized_function(x, D, mu, sigma))\n",
        "    return function_class\n",
        "\n",
        "# Maximum Likelihood Estimation (MLE)\n",
        "def mle(X, D):\n",
        "    function_class = transition_probabilities(X, D)\n",
        "    n = len(X)\n",
        "    P_hat = np.zeros((n, len(D[0])))\n",
        "\n",
        "    for i, x in enumerate(X):\n",
        "        log_likelihood = np.zeros(len(function_class))\n",
        "        for j, p in enumerate(function_class):\n",
        "            log_likelihood[j] = np.sum(np.log(p(x, D[i, :])))\n",
        "        best_function_idx = np.argmax(log_likelihood)\n",
        "        P_hat[i, :] = function_class[best_function_idx](x, D[i, :])\n",
        "\n",
        "    return P_hat\n",
        "\n",
        "# Value Iteration\n",
        "def value_iteration2(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon):\n",
        "    n_y = len(Y)\n",
        "    n_x = len(X)\n",
        "    P_hat = mle(X, D)  # estimate P_hat using Maximum Likelihood Estimation\n",
        "    V_yx = np.zeros((n_y, n_x))\n",
        "    V_y = np.zeros(n_y)\n",
        "    V_next_yx = np.zeros_like(V_yx)\n",
        "    V_next_y = np.zeros_like(V_y)\n",
        "    policy = np.zeros_like(V_yx, dtype=int)\n",
        "\n",
        "    # Store the value functions and policies at each iteration\n",
        "    value_history = []\n",
        "    policy_history = []\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        for i, y in enumerate(Y):\n",
        "            for j, x in enumerate(X):\n",
        "                min_val = np.inf\n",
        "                min_a = None\n",
        "\n",
        "                for a in A:\n",
        "                    cost = c * a\n",
        "                    y_plus = y + a - D[j, :]\n",
        "\n",
        "                    # Clip Y_plus values to stay within the Y range\n",
        "                    y_plus = np.clip(y_plus, Y[0], Y[-1])\n",
        "\n",
        "                    # Find the index of Y_plus values in the Y array\n",
        "                    y_plus_idx = np.searchsorted(Y, y_plus)\n",
        "\n",
        "                    # Calculate the expected value of the next state, weighted by P(D|X)\n",
        "                    next_state_value = V_yx[y_plus_idx, j]\n",
        "                    weighted_next_state_value = np.sum(P_hat[j, :] * next_state_value)\n",
        "\n",
        "                    value = cost + q(y, D[j, :].mean(), h, b) + gamma * weighted_next_state_value\n",
        "\n",
        "                    if value < min_val:\n",
        "                        min_val = value\n",
        "                        min_a = a\n",
        "\n",
        "                V_next_yx[i, j] = min_val\n",
        "                policy[i, j] = min_a\n",
        "\n",
        "            V_next_y[i] = np.mean(V_next_yx[i, :])\n",
        "\n",
        "        value_history.append(V_next_y.copy())\n",
        "        policy_history.append(policy.copy())\n",
        "\n",
        "        if np.abs(V_next_y - V_y).max() < epsilon:\n",
        "            break\n",
        "\n",
        "        V_yx = np.copy(V_next_yx)\n",
        "        V_y = np.copy(V_next_y)\n",
        "\n",
        "    return V_y, policy, value_history, policy_history\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration2(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon):\n",
        "    n_y = len(Y)\n",
        "    n_x = len(X)\n",
        "    P_hat = mle(X, D)  # estimate P_hat using Maximum Likelihood Estimation\n",
        "    V_yx = np.zeros((n_y, n_x))\n",
        "    V_y = np.zeros(n_y)\n",
        "    V_next_yx = np.zeros_like(V_yx)\n",
        "    V_next_y = np.zeros_like(V_y)\n",
        "    policy = np.zeros_like(V_yx, dtype=int)\n",
        "\n",
        "    # Store the value functions and policies at each iteration\n",
        "    value_history = []\n",
        "    policy_history = []\n",
        "\n",
        "    time_history = []\n",
        "    for i in range(max_iterations):\n",
        "        start_time = time.time()\n",
        "        for i, y in enumerate(Y):\n",
        "            for j, x in enumerate(X):\n",
        "                min_val = np.inf\n",
        "                min_a = None\n",
        "\n",
        "                for a in A:\n",
        "                    cost = c * a\n",
        "                    y_plus = y + a - D[j, :]\n",
        "\n",
        "                    # Clip Y_plus values to stay within the Y range\n",
        "                    y_plus = np.clip(y_plus, Y[0], Y[-1])\n",
        "\n",
        "                    # Find the index of Y_plus values in the Y array\n",
        "                    y_plus_idx = np.searchsorted(Y, y_plus)\n",
        "\n",
        "                    # Calculate the expected value of the next state, weighted by P(D|X)\n",
        "                    next_state_value = V_yx[y_plus_idx, j]\n",
        "                    weighted_next_state_value = np.sum(P_hat[j, :] * next_state_value)\n",
        "\n",
        "                    value = cost + q(y, D[j, :].mean(), h, b) + gamma * weighted_next_state_value\n",
        "\n",
        "                    if value < min_val:\n",
        "                        min_val = value\n",
        "                        min_a = a\n",
        "\n",
        "                V_next_yx[i, j] = min_val\n",
        "                policy[i, j] = min_a\n",
        "\n",
        "            V_next_y[i] = np.mean(V_next_yx[i, :])\n",
        "\n",
        "        value_history.append(V_next_y.copy())\n",
        "        policy_history.append(policy.copy())\n",
        "\n",
        "        if np.abs(V_next_y - V_y).max() < epsilon:\n",
        "            break\n",
        "\n",
        "        V_yx = np.copy(V_next_yx)\n",
        "        V_y = np.copy(V_next_y)\n",
        "        end_time = time.time()\n",
        "        time_history.append(end_time - start_time)\n",
        "        # Record value and append to value_history\n",
        "        value_history.append(V.copy())\n",
        "\n",
        "    return V_y, policy, value_history, policy_history, time_history\n"
      ],
      "metadata": {
        "id": "DbDpvf5dYhb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "def value_iteration0(D, Y, A, gamma, h, b, c, max_iterations, epsilon):\n",
        "    n_y = len(Y)\n",
        "    V_y = np.zeros(n_y)\n",
        "    V_next_y = np.zeros_like(V_y)\n",
        "    policy = np.zeros_like(V_y, dtype=int)\n",
        "\n",
        "    # Store the value functions, policies and time spent at each iteration\n",
        "    value_history = []\n",
        "    policy_history = []\n",
        "    time_history = []\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        start_time = time.time()  # Start time measurement\n",
        "        for i, y in enumerate(Y):\n",
        "            min_val = np.inf\n",
        "            min_a = None\n",
        "\n",
        "            for a in A:\n",
        "                cost = c * a\n",
        "                y_plus = y + a - D\n",
        "\n",
        "                # Clip Y_plus values to stay within the Y range\n",
        "                y_plus = np.clip(y_plus, Y[0], Y[-1])\n",
        "\n",
        "                # Find the index of Y_plus values in the Y array\n",
        "                y_plus_idx = np.searchsorted(Y, y_plus)\n",
        "\n",
        "                # Calculate the expected value of the next state\n",
        "                next_state_value = V_y[y_plus_idx]\n",
        "                expected_next_state_value = np.mean(next_state_value)\n",
        "\n",
        "                value = cost + q(y, np.mean(D), h, b) + gamma * expected_next_state_value\n",
        "\n",
        "                if value < min_val:\n",
        "                    min_val = value\n",
        "                    min_a = a\n",
        "\n",
        "            V_next_y[i] = min_val\n",
        "            policy[i] = min_a\n",
        "\n",
        "        time_spent = time.time() - start_time  # Measure time spent\n",
        "        time_history.append(time_spent)\n",
        "\n",
        "        value_history.append(V_next_y.copy())\n",
        "        policy_history.append(policy.copy())\n",
        "\n",
        "        if np.abs(V_next_y - V_y).max() < epsilon:\n",
        "            break\n",
        "\n",
        "        V_y = np.copy(V_next_y)\n",
        "\n",
        "    return V_y, policy, value_history, policy_history, time_history\n",
        "\n",
        "\n",
        "def value_iteration2(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon):\n",
        "    n_y = len(Y)\n",
        "    n_x = len(X)\n",
        "    P_hat = mle(X, D)  # estimate P_hat using Maximum Likelihood Estimation\n",
        "    V_yx = np.zeros((n_y, n_x))\n",
        "    V_y = np.zeros(n_y)\n",
        "    V_next_yx = np.zeros_like(V_yx)\n",
        "    V_next_y = np.zeros_like(V_y)\n",
        "    policy = np.zeros_like(V_yx, dtype=int)\n",
        "\n",
        "    # Store the value functions, policies and time spent at each iteration\n",
        "    value_history = []\n",
        "    policy_history = []\n",
        "    time_history = []\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        start_time = time.time()  # Start time measurement\n",
        "        for i, y in enumerate(Y):\n",
        "            for j, x in enumerate(X):\n",
        "                min_val = np.inf\n",
        "                min_a = None\n",
        "\n",
        "                for a in A:\n",
        "                    cost = c * a\n",
        "                    y_plus = y + a - D[j, :]\n",
        "\n",
        "                    # Clip Y_plus values to stay within the Y range\n",
        "                    y_plus = np.clip(y_plus, Y[0], Y[-1])\n",
        "\n",
        "                    # Find the index of Y_plus values in the Y array\n",
        "                    y_plus_idx = np.searchsorted(Y, y_plus)\n",
        "\n",
        "                    # Calculate the expected value of the next state, weighted by P(D|X)\n",
        "                    next_state_value = V_yx[y_plus_idx, j]\n",
        "                    weighted_next_state_value = np.sum(P_hat[j, :] * next_state_value)\n",
        "\n",
        "                    value = cost + q(y, D[j, :].mean(), h, b) + gamma * weighted_next_state_value\n",
        "\n",
        "                    if value < min_val:\n",
        "                        min_val = value\n",
        "                        min_a = a\n",
        "\n",
        "                V_next_yx[i, j] = min_val\n",
        "                policy[i, j] = min_a\n",
        "\n",
        "            V_next_y[i] = np.mean(V_next_yx[i, :])\n",
        "\n",
        "        time_spent = time.time() - start_time  # Measure time spent\n",
        "        time_history.append(time_spent)\n",
        "\n",
        "        value_history.append(V_next_y.copy())\n",
        "        policy_history.append(policy.copy())\n",
        "\n",
        "        if np.abs(V_next_y - V_y).max() < epsilon:\n",
        "            break\n",
        "\n",
        "        V_yx = np.copy(V_next_yx)\n",
        "        V_y = np.copy(V_next_y)\n",
        "\n",
        "    return V_y, policy, value_history, policy_history, time_history\n"
      ],
      "metadata": {
        "id": "KjMNfd93FDU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "def value_iteration0(D, Y, A, gamma, h, b, c, max_iterations, epsilon):\n",
        "    n_y = len(Y)\n",
        "    V_y = np.zeros(n_y)\n",
        "    V_next_y = np.zeros_like(V_y)\n",
        "    policy = np.zeros_like(V_y, dtype=int)\n",
        "\n",
        "    # Store the value functions, policies and cumulative time at each iteration\n",
        "    value_history = []\n",
        "    policy_history = []\n",
        "    cumulative_time_history = []\n",
        "\n",
        "    cumulative_time = 0.0\n",
        "    for iteration in range(max_iterations):\n",
        "        start_time = time.time()\n",
        "        for i, y in enumerate(Y):\n",
        "            min_val = np.inf\n",
        "            min_a = None\n",
        "\n",
        "            for a in A:\n",
        "                cost = c * a\n",
        "                y_plus = y + a - D\n",
        "\n",
        "                # Clip Y_plus values to stay within the Y range\n",
        "                y_plus = np.clip(y_plus, Y[0], Y[-1])\n",
        "\n",
        "                # Find the index of Y_plus values in the Y array\n",
        "                y_plus_idx = np.searchsorted(Y, y_plus)\n",
        "\n",
        "                # Calculate the expected value of the next state\n",
        "                next_state_value = V_y[y_plus_idx]\n",
        "                expected_next_state_value = np.mean(next_state_value)\n",
        "\n",
        "                value = cost + q(y, np.mean(D), h, b) + gamma * expected_next_state_value\n",
        "\n",
        "                if value < min_val:\n",
        "                    min_val = value\n",
        "                    min_a = a\n",
        "\n",
        "            V_next_y[i] = min_val\n",
        "            policy[i] = min_a\n",
        "\n",
        "        cumulative_time += time.time() - start_time\n",
        "        cumulative_time_history.append(cumulative_time)\n",
        "\n",
        "        value_history.append(V_next_y.copy())\n",
        "        policy_history.append(policy.copy())\n",
        "\n",
        "        if np.abs(V_next_y - V_y).max() < epsilon:\n",
        "            break\n",
        "\n",
        "        V_y = np.copy(V_next_y)\n",
        "\n",
        "    return V_y, policy, value_history, policy_history, cumulative_time_history\n",
        "\n",
        "\n",
        "def value_iteration2(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon):\n",
        "    n_y = len(Y)\n",
        "    n_x = len(X)\n",
        "    P_hat = mle(X, D)  # estimate P_hat using Maximum Likelihood Estimation\n",
        "    V_yx = np.zeros((n_y, n_x))\n",
        "    V_y = np.zeros(n_y)\n",
        "    V_next_yx = np.zeros_like(V_yx)\n",
        "    V_next_y = np.zeros_like(V_y)\n",
        "    policy = np.zeros_like(V_yx, dtype=int)\n",
        "\n",
        "    # Store the value functions, policies and cumulative time at each iteration\n",
        "    value_history = []\n",
        "    policy_history = []\n",
        "    cumulative_time_history = []\n",
        "\n",
        "    cumulative_time = 0.0\n",
        "    for iteration in range(max_iterations):\n",
        "        start_time = time.time()\n",
        "        for i, y in enumerate(Y):\n",
        "            for j, x in enumerate(X):\n",
        "                min_val = np.inf\n",
        "                min_a = None\n",
        "\n",
        "                for a in A:\n",
        "                    cost = c * a\n",
        "                    y_plus = y + a - D[j, :]\n",
        "\n",
        "                    # Clip Y_plus values to stay within the Y range\n",
        "                    y_plus = np.clip(y_plus, Y[0], Y[-1])\n",
        "\n",
        "                    # Find the index of Y_plus values in the Y array\n",
        "                    y_plus_idx = np.searchsorted(Y, y_plus)\n",
        "\n",
        "                    # Calculate the expected value of the next state, weighted by P(D|X)\n",
        "                    next_state_value = V_yx[y_plus_idx, j]\n",
        "                    weighted_next_state_value = np.sum(P_hat[j, :] * next_state_value)\n",
        "\n",
        "                    value = cost + q(y, D[j, :].mean(), h, b) + gamma * weighted_next_state_value\n",
        "\n",
        "                    if value < min_val:\n",
        "                        min_val = value\n",
        "                        min_a = a\n",
        "\n",
        "                V_next_yx[i, j] = min_val\n",
        "                policy[i, j] = min_a\n",
        "\n",
        "            V_next_y[i] = np.mean(V_next_yx[i, :])\n",
        "\n",
        "        cumulative_time += time.time() - start_time\n",
        "        cumulative_time_history.append(cumulative_time)\n",
        "\n",
        "        value_history.append(V_next_y.copy())\n",
        "        policy_history.append(policy.copy())\n",
        "\n",
        "        if np.abs(V_next_y - V_y).max() < epsilon:\n",
        "            break\n",
        "\n",
        "        V_yx = np.copy(V_next_yx)\n",
        "        V_y = np.copy(V_next_y)\n",
        "\n",
        "    return V_y, policy, value_history, policy_history, cumulative_time_history\n"
      ],
      "metadata": {
        "id": "CIWubqzKKFv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_cva_T9dfNu"
      },
      "source": [
        "## gMDP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KOI4CoJdeio"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Cost function q\n",
        "def q(y, d, h, b):\n",
        "    return h * max(0, -y) + b * max(0, y)\n",
        "\n",
        "def value_iteration0(D, Y, A, gamma, h, b, c, max_iterations, epsilon):\n",
        "    n_y = len(Y)\n",
        "    V_y = np.zeros(n_y)\n",
        "    V_next_y = np.zeros_like(V_y)\n",
        "    policy = np.zeros_like(V_y, dtype=int)\n",
        "\n",
        "    # Store the value functions and policies at each iteration\n",
        "    value_history = []\n",
        "    policy_history = []\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        for i, y in enumerate(Y):\n",
        "            min_val = np.inf\n",
        "            min_a = None\n",
        "\n",
        "            for a in A:\n",
        "                cost = c * a\n",
        "                y_plus = y + a - D\n",
        "\n",
        "                # Clip Y_plus values to stay within the Y range\n",
        "                y_plus = np.clip(y_plus, Y[0], Y[-1])\n",
        "\n",
        "                # Find the index of Y_plus values in the Y array\n",
        "                y_plus_idx = np.searchsorted(Y, y_plus)\n",
        "\n",
        "                # Calculate the expected value of the next state\n",
        "                next_state_value = V_y[y_plus_idx]\n",
        "                expected_next_state_value = np.mean(next_state_value)\n",
        "\n",
        "                value = cost + q(y, np.mean(D), h, b) + gamma * expected_next_state_value\n",
        "\n",
        "                if value < min_val:\n",
        "                    min_val = value\n",
        "                    min_a = a\n",
        "\n",
        "            V_next_y[i] = min_val\n",
        "            policy[i] = min_a\n",
        "\n",
        "        value_history.append(V_next_y.copy())\n",
        "        policy_history.append(policy.copy())\n",
        "\n",
        "        if np.abs(V_next_y - V_y).max() < epsilon:\n",
        "            break\n",
        "\n",
        "        V_y = np.copy(V_next_y)\n",
        "\n",
        "    return V_y, policy, value_history, policy_history\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration0(D, Y, A, gamma, h, b, c, max_iterations, epsilon):\n",
        "    n_y = len(Y)\n",
        "    V_y = np.zeros(n_y)\n",
        "    V_next_y = np.zeros_like(V_y)\n",
        "    policy = np.zeros_like(V_y, dtype=int)\n",
        "\n",
        "    # Store the value functions and policies at each iteration\n",
        "    value_history = []\n",
        "    policy_history = []\n",
        "\n",
        "    time_history = []\n",
        "    for i in range(max_iterations):\n",
        "        start_time = time.time()\n",
        "        for i, y in enumerate(Y):\n",
        "            min_val = np.inf\n",
        "            min_a = None\n",
        "\n",
        "            for a in A:\n",
        "                cost = c * a\n",
        "                y_plus = y + a - D\n",
        "\n",
        "                # Clip Y_plus values to stay within the Y range\n",
        "                y_plus = np.clip(y_plus, Y[0], Y[-1])\n",
        "\n",
        "                # Find the index of Y_plus values in the Y array\n",
        "                y_plus_idx = np.searchsorted(Y, y_plus)\n",
        "\n",
        "                # Calculate the expected value of the next state\n",
        "                next_state_value = V_y[y_plus_idx]\n",
        "                expected_next_state_value = np.mean(next_state_value)\n",
        "\n",
        "                value = cost + q(y, np.mean(D), h, b) + gamma * expected_next_state_value\n",
        "\n",
        "                if value < min_val:\n",
        "                    min_val = value\n",
        "                    min_a = a\n",
        "\n",
        "            V_next_y[i] = min_val\n",
        "            policy[i] = min_a\n",
        "\n",
        "        value_history.append(V_next_y.copy())\n",
        "        policy_history.append(policy.copy())\n",
        "\n",
        "        if np.abs(V_next_y - V_y).max() < epsilon:\n",
        "            break\n",
        "\n",
        "        V_y = np.copy(V_next_y)\n",
        "\n",
        "        end_time = time.time()\n",
        "        time_history.append(end_time - start_time)\n",
        "\n",
        "    return V_y, policy, value_history, policy_history, time_history\n"
      ],
      "metadata": {
        "id": "8J_rpQ_XYDAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzJ2B9TYe2t8"
      },
      "source": [
        "## Compare VI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpU9lezNjtPW"
      },
      "outputs": [],
      "source": [
        "# Define your parameter ranges\n",
        "h_values = [1, 2]\n",
        "b_values = [1, 2]\n",
        "c_values = [0, 1]\n",
        "gamma_values = [0.1, 0.9]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXeNKkIihhcO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# Assuming the same parameters for all three functions\n",
        "# X = ...\n",
        "# D = ...\n",
        "# Y = ...\n",
        "# A = ...\n",
        "# gamma = ...\n",
        "# h = ...\n",
        "# b = ...\n",
        "# c = ...\n",
        "# max_iterations = ...\n",
        "# epsilon = ...\n",
        "# P_hat = ...\n",
        "\n",
        "# Run the first value iteration function\n",
        "start = time.time()\n",
        "V_y1, policy1, value_history1, policy_history1 = value_iteration(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon, P_hat)\n",
        "end = time.time()\n",
        "print(f\"Time for first value iteration: {end - start} seconds\")\n",
        "\n",
        "# Run the second value iteration function\n",
        "start = time.time()\n",
        "V_y2, policy2, value_history2, policy_history2 = value_iteration0(D, Y, A, gamma, h, b, c, max_iterations, epsilon)\n",
        "end = time.time()\n",
        "print(f\"Time for second value iteration: {end - start} seconds\")\n",
        "\n",
        "# Run the third value iteration function\n",
        "start = time.time()\n",
        "V_y3, policy3, value_history3, policy_history3 = value_iteration2(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon)\n",
        "end = time.time()\n",
        "print(f\"Time for third value iteration: {end - start} seconds\")\n",
        "\n",
        "# Plot optimal value function versus state y for all three functions\n",
        "plt.figure()\n",
        "plt.plot(Y, V_y1, label='value_iteration')\n",
        "plt.plot(Y, V_y2, label='value_iteration0')\n",
        "plt.plot(Y, V_y3, label='value_iteration2')\n",
        "plt.xlabel('State y')\n",
        "plt.ylabel('Optimal Value Function')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot value history versus iteration number for all three functions\n",
        "plt.figure()\n",
        "plt.plot([np.max(v) for v in value_history1], label='value_iteration')\n",
        "plt.plot([np.max(v) for v in value_history2], label='value_iteration0')\n",
        "plt.plot([np.max(v) for v in value_history3], label='value_iteration2')\n",
        "plt.xlabel('Iteration number')\n",
        "plt.ylabel('Value History')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPZRI7oxkXSh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define directory to save results\n",
        "fig_dir = '/content'\n",
        "if not os.path.exists(fig_dir):\n",
        "    os.makedirs(fig_dir)\n",
        "\n",
        "# Iterate over all parameter combinations of h, b, c, gamma, and X\n",
        "for h in h_values:\n",
        "    for b in b_values:\n",
        "        for c in c_values:\n",
        "            for gamma in gamma_values:\n",
        "                # Run both value iterations\n",
        "                V_y1, policy1, value_history1, policy_history1 = value_iteration(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon, P_hat)\n",
        "                V_y2, policy2, value_history2, policy_history2 = value_iteration2(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon)\n",
        "\n",
        "                # Compare outcomes\n",
        "                difference_in_values = np.abs(V_y1 - V_y2)\n",
        "                difference_in_policies = np.abs(policy1 - policy2)\n",
        "\n",
        "                max_diff_values = np.max(difference_in_values)\n",
        "                max_diff_policies = np.max(difference_in_policies)\n",
        "\n",
        "                print(\"Max difference in values: \", max_diff_values)\n",
        "                print(\"Max difference in policies: \", max_diff_policies)\n",
        "\n",
        "                # Write results to file\n",
        "                with open(os.path.join(fig_dir, 'results.txt'), 'a') as f:\n",
        "                    f.write(f\"For h={h}, b={b}, c={c}, gamma={gamma}:\\n\")\n",
        "                    f.write(f\"Max difference in values: {max_diff_values}\\n\")\n",
        "                    f.write(f\"Max difference in policies: {max_diff_policies}\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW-wEo_qgb31"
      },
      "source": [
        "## compare q-learning and VI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvYq8yhxtk2m"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting Value Histories\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# For simplicity, let's just compare the average value over states at each iteration\n",
        "avg_value_history_vi1 = [np.mean(values) for values in value_history_vi1]\n",
        "avg_value_history_vi2 = [np.mean(values) for values in value_history_vi2]\n",
        "# avg_value_history_ql = [np.mean(values) for values in value_history_ql]\n",
        "\n",
        "plt.plot(avg_value_history_vi1, label='Value Iteration 1')\n",
        "plt.plot(avg_value_history_vi2, label='Value Iteration 2')\n",
        "# plt.plot(avg_value_history_ql, label='Q-Learning')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Average Value')\n",
        "plt.title('Average Value Over Iterations')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Plotting Policy Histories\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Here we plot the number of changes in the policy at each iteration\n",
        "policy_changes_vi1 = [np.sum(policy_history_vi1[i] != policy_history_vi1[i-1]) if i > 0 else 0 for i in range(len(policy_history_vi1))]\n",
        "policy_changes_vi2 = [np.sum(policy_history_vi2[i] != policy_history_vi2[i-1]) if i > 0 else 0 for i in range(len(policy_history_vi2))]\n",
        "\n",
        "plt.plot(policy_changes_vi1, label='Value Iteration 1')\n",
        "plt.plot(policy_changes_vi2, label='Value Iteration 2')\n",
        "# plt.plot(policy_changes_ql, label='Q-Learning')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Number of Policy Changes')\n",
        "plt.title('Policy Changes Over Iterations')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWYGxQuXCAzy"
      },
      "outputs": [],
      "source": [
        "def plot_convergence(x, i, value_history1, policy_history1, value_history2, policy_history2, h, b, c, gamma, fig_dir):\n",
        "    # Value function convergence plot\n",
        "    plt.figure()\n",
        "    for j, (V_y_iter1, V_y_iter2) in enumerate(zip(value_history1, value_history2)):\n",
        "        plt.plot(Y, V_y_iter1[:, i], label=f\"Iteration {j} (VI 1)\")\n",
        "        plt.plot(Y, V_y_iter2[:, i], label=f\"Iteration {j} (VI 2)\", linestyle='--')\n",
        "\n",
        "    plt.xlabel(\"Inventory Level\")\n",
        "    plt.ylabel(\"Value Function\")\n",
        "    plt.title(f\"Value Function Convergence for X{i} = {x}, h={h}, b={b}, c={c}, gamma={gamma}\")\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"{fig_dir}/value_convergence_X{i}_h={h}_b={b}_c={c}_gamma={gamma}.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Policy convergence plot\n",
        "    plt.figure()\n",
        "    for j, (policy_iter1, policy_iter2) in enumerate(zip(policy_history1, policy_history2)):\n",
        "        plt.plot(Y, policy_iter1[:, i], label=f\"Iteration {j} (VI 1)\")\n",
        "        plt.plot(Y, policy_iter2[:, i], label=f\"Iteration {j} (VI 2)\", linestyle='--')\n",
        "\n",
        "    plt.xlabel(\"Inventory Level\")\n",
        "    plt.ylabel(\"Order Quantity\")\n",
        "    plt.title(f\"Policy Convergence for X{i} = {x}, h={h}, b={b}, c={c}, gamma={gamma}\")\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"{fig_dir}/policy_convergence_X{i}_h={h}_b={b}_c={c}_gamma={gamma}.png\")\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to plot the value and policy histories\n",
        "def plot_convergence2(i, value_history0, value_history1, value_history2, h, b, c, gamma, fig_dir):\n",
        "    # Value function convergence plot\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    max_values0 = [np.max(V) for V in value_history0]\n",
        "    max_values1 = [np.max(V) for V in value_history1]\n",
        "    max_values2 = [np.max(V) for V in value_history2]\n",
        "    plt.plot(range(len(value_history0)), max_values0, label=f\"VI 0\")\n",
        "    plt.plot(range(len(value_history1)), max_values1, label=f\"VI 1\")\n",
        "    plt.plot(range(len(value_history2)), max_values2, label=f\"VI 2\", linestyle='--')\n",
        "\n",
        "    plt.xlabel(\"Iteration Number\")\n",
        "    plt.ylabel(\"Maximum Value Function\")\n",
        "    plt.title(f\"Value Function Convergence for X{i}, h={h}, b={b}, c={c}, gamma={gamma}\")\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"{fig_dir}/value_convergence_X{i}_h={h}_b={b}_c={c}_gamma={gamma}.png\")\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "Jm67vd4ePDGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Assuming that you already defined these values somewhere in your code\n",
        "# X, D, Y, A, gamma_values, h_values, b_values, c_values, max_iterations, epsilon, P_hat\n",
        "\n",
        "# Define a function to plot the value and policy histories\n",
        "def plot_convergence(i, value_history1, value_history2, value_history0, h, b, c, gamma, fig_dir):\n",
        "    # Value function convergence plot\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    max_values1 = [np.max(V) for V in value_history1]\n",
        "    max_values2 = [np.max(V) for V in value_history2]\n",
        "    max_values0 = [np.max(V) for V in value_history0]\n",
        "    plt.plot(range(len(value_history1)), max_values1, label=f\"VI 1\")\n",
        "    plt.plot(range(len(value_history2)), max_values2, label=f\"VI 2\", linestyle='--')\n",
        "    plt.plot(range(len(value_history0)), max_values0, label=f\"VI 0\", linestyle='-.')\n",
        "\n",
        "    plt.xlabel(\"Iteration Number\")\n",
        "    plt.ylabel(\"Maximum Value Function\")\n",
        "    plt.title(f\"Value Function Convergence for X{i}, h={h}, b={b}, c={c}, gamma={gamma}\")\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"{fig_dir}/value_convergence_X{i}_h={h}_b={b}_c={c}_gamma={gamma}.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# Iterate over all parameter combinations of h, b, c, gamma, and X\n",
        "for h in h_values:\n",
        "    for b in b_values:\n",
        "        for c in c_values:\n",
        "            for gamma in gamma_values:\n",
        "                subdir = f\"h={h}_b={b}_c={c}_gamma={gamma}\"\n",
        "                os.makedirs(subdir, exist_ok=True)\n",
        "\n",
        "                results_dir = f\"{subdir}/results\"\n",
        "                os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "                fig_dir = f\"{subdir}/figures\"\n",
        "                os.makedirs(fig_dir, exist_ok=True)\n",
        "\n",
        "                for i, x in enumerate(X):\n",
        "                    # Run the first value iteration function\n",
        "                    V_y1, policy1, value_history1, policy_history1 = value_iteration(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon, P_hat)\n",
        "                    # Run the second value iteration function\n",
        "                    V_y2, policy2, value_history2, policy_history2 = value_iteration2(X, D, Y, A, gamma, h, b, c, max_iterations, epsilon)\n",
        "                    # Run the third value iteration function\n",
        "                    V_y0, policy0, value_history0, policy_history0 = value_iteration0(D, Y, A, gamma, h, b, c, max_iterations, epsilon)\n",
        "\n",
        "                    np.save(f\"{results_dir}/V_y1_X{i}_h={h}_b={b}_c={c}_gamma={gamma}.npy\", V_y1)\n",
        "                    np.save(f\"{results_dir}/policy1_X{i}_h={h}_b={b}_c={c}_gamma={gamma}.npy\", policy1)\n",
        "                    np.save(f\"{results_dir}/V_y2_X{i}_h={h}_b={b}_c={c}_gamma={gamma}.npy\", V_y2)\n",
        "                    np.save(f\"{results_dir}/policy2_X{i}_h={h}_b={b}_c={c}_gamma={gamma}.npy\", policy2)\n",
        "                    np.save(f\"{results_dir}/V_y0_X{i}_h={h}_b={b}_c={c}_gamma={gamma}.npy\", V_y0)\n",
        "                    np.save(f\"{results_dir}/policy0_X{i}_h={h}_b={b}_c={c}_gamma={gamma}.npy\", policy0)\n",
        "\n",
        "                    plot_convergence(i, value_history1, value_history2, value_history0, h, b, c, gamma, fig_dir)\n"
      ],
      "metadata": {
        "id": "ZztzcQagVe7o"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMA8lFfyuo1TadWsPrQiuwE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}